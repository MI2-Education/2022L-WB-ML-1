"data.frame" %in% class(data),
# Support only simple objectives
(objective %in% c("regression", "binary", "multiclass")),
is.null(params$objective),
# Filled in by this function
is.null(params$num_class),
is.null(params$categorical_feature)
)
# Convert all <character> columns to <factor> if requested.
# The encoding is likely to be dependent on order, which is not desirable.
if (autofactor) {
data <- lgbm_character_to_factor(data)
} else {
lgbm_assert_no_characters(data)
}
# Convert all <factor> columns to <numeric>.
categorical_columns <- sapply(data, is.factor)
data <- lgbm_factor_to_numeric(data)
# Force conversion to <numeric> if requested
if (forceconvert) {
data <- data.frame(lapply(data, as.numeric))
} else {
lgbm_assert_all_numeric(data)
}
# Fill in parameters based on
if (objective == "regression") {
params$objective = "regression"
} else if (objective == "multiclass") {
params$objective = "multiclass"
params$num_classes = length(unique(data[, target]))
# LGB expects a numeric vector starting from 0, R encodes factors from 1
data[, target] <- data[, target] - 1L
} else {
params$objective = "binary"
}
# Convert dataframe to lgb.dataset
dataset <- lgbm_df_to_train(data, target, categorical_columns)
# Train model
model <- lgb.train(
params = params,
data = dataset
)
return(model)
}
lightGBM_predict <- function(
model,
data,
target=NULL,
autofactor=FALSE,
postprocess=TRUE
){
# Drop target if provided
if (!is.null(target)) {
data_target <- data[, target]
data <- data[, !(names(data) %in% target)]
}
# Convert all <character> columns to <factor> if requested.
# The encoding is likely to be dependent on order, which is not desirable.
if (autofactor) {
data <- lgbm_character_to_factor(data)
} else {
lgbm_assert_no_characters(data)
}
# Convert all <factor> columns to <numeric>.
categorical_columns <- sapply(data, is.factor)
data <- lgbm_factor_to_numeric(data)
lgbm_assert_all_numeric(data)
test <- as.matrix(data)
prediction <- predict(model, test)
if (postprocess && model$params$objective == "multiclass") {
proba_matrix <- matrix(prediction, nrow = model$params$num_classes)
proba_matrix <- t(proba_matrix)
rownames(proba_matrix) <- rownames(data)
if (is.null(target) || !is.factor(data_target)) {
warning(
"Postprocessing requested for multiclass objective, ",
"but `target` not provided or `data$target` is not a factor. ",
"Column names won't be set."
)
}
else {
colnames(proba_matrix) <- levels(data_target)
}
return(proba_matrix)
}
return(prediction)
}
lgbm_test_regression <- function() {
set.seed(123)
train <- train
model <- lightGBM_function(
train,
"price_log",
"regression"
)
test <- test[,-c(21)]
predicted <- lightGBM_predict(model, test)
actual <- test$price_log
mean((predicted - actual)^2)
}
lgbm_test_regression()
lgbm_assert_no_characters <- function(df) {
if (any(sapply(df, is.character))) {
stop(
"Character columns present and `autofactor` is set to FALSE. ",
"Character columns: ",
paste0(colnames(df[,sapply(df, is.character)]), collapse=", ")
)
}
}
lgbm_assert_all_numeric <- function(df) {
if (!all(sapply(df, is.numeric))) {
stop(
"Non-numeric columns present after pre-processing. Problem columns: ",
paste0(colnames(df[,! sapply(df, is.numeric)]), collapse=", ")
)
}
}
lgbm_character_to_factor <- function(df) {
df[sapply(df, is.character)] <- lapply(
df[sapply(df, is.character)],
as.factor
)
df
}
lgbm_factor_to_numeric <- function(df) {
df[sapply(df, is.factor)] <- lapply(
df[sapply(df, is.factor)],
as.numeric
)
df
}
lgbm_df_to_train <- function(df, target, categorical_columns) {
target_values <- df[, target]
features <- !(names(df) %in% target)
data_features <- df[ , features]
data_matrix <- as.matrix(data_features[, 1:ncol(data_features)])
return(
lgb.Dataset(
data = data_matrix,
label = target_values,
free_raw_data = FALSE,
categorical_feature = categorical_columns
)
)
}
lgbm_test_regression()
model <- lightGBM_function(
train,
"price_log",
"regression"
)
test1 <- test[,-c(21)]
predicted <- lightGBM_predict(model, test1)
predicted
actual <- test$price_log
mean((predicted - actual)^2)
sqrt(mean((predicted - actual)^2))
library(dplyr)
library(zoo)
library(tidyr)
library(naniar)
library(caret)
library(devtools)
library(ranger)
library(Metrics)
to_DataFrame <- function(data, target) {
if (any(class(data) == "list") | any(class(data) == "matrix") | any(class(data) == "data.table")) {
data <- as.data.frame(data)
}
if (!any(class(data) %in% c("data.frame", "matrix", "data.table", "list"))) {
stop("Data type is invalid")
}
if (nrow(data) == 0 | ncol(data) == 1) {
stop("Data frame is empty or too small to create model.")
}
#if (!is.null(target) &
#    (!is.character(target) | !(target %in% colnames(data)))) {
#  stop("Invalid 'target'")
#}
return(data)
}
#' @param target        string, name of the target column
#' @param missing_num   additional parameter, value which describes missing value
#' for 'numeric' observations (other than NA)
#' @param missing_cat   additional parameter, value which describes missing value
#' for 'character' observations (other than NA)
#'
#' @return              returns data.frame object
#' @export
#'
#' @examples
null_exterminator <- function(data, target, missing_num = FALSE, missing_cat = FALSE){
if (target %in% colnames(data)) {
if (any(is.na(data[target]))) {
paste("missing values in column 'target': ",
sum(is.na(data[target])) / nrow(data) * 100 , "%.")
data <- data %>% drop_na(target)
}
}
if (!any(data == missing_num) | missing_num == FALSE) {
print("Not covered missing numerical values")
} else if (any(data == missing_num)) {
data <- data %>%
replace_with_na_if(.predicate = is.numeric,
condition = ~ .x == missing_num)
}
if (!any(data == missing_cat) | missing_cat == FALSE) {
print("Not covered missing nominal values")
} else {
data <- data %>%
replace_with_na_if(.predicate = is.character,
condition = ~ .x == missing_cat)
}
data <- data %>% mutate_if(is.numeric , zoo::na.aggregate)
calc_mode <- function(x) {
distinct_values <- unique(x)
distinct_tabulate <- tabulate(match(x, distinct_values))
distinct_values[which.max(distinct_tabulate)]
}
data <- data %>%
mutate(across(everything(), ~ replace_na(.x, calc_mode(.x))))
return(data)
}
#' Scales and standardizes the columns provided as a vector of column indexes
#' or column names.
#'
#' @param data      data.frame to scale
#' @param to_scale  vector of column indexes or column names to scale
#'
#' @return          data.frame with scaled columns
#' @export
#'
#' @examples
scale_continuous <- function(data, to_scale) {
if (!is.na(to_scale)) {
data[to_scale] <- lapply(data[to_scale],
function(x) c(scale(x, center = FALSE)))
return(data)
} else{
print("Can't scale the data without continuous values")
return(data)
}
}
#' Binarizes dataframe columns which are characters and have only two unique
#' values inside of themselves
#'
#' @param data      data.frame to binarize
#'
#' @return          data.frame with binarized proper columns
#' @export
#'
#' @examples
binarize_categorical <- function(data){
for (i in colnames(data)) {
if (typeof(data[[i]]) == 'character') {
levels <- c()
for (el in unique(data[i])) {
levels <- c(levels, el)
}
if (length(levels) == 2) {
for (j in 1:length(levels)) {
data[[i]][data[[i]] == levels[j]] <- (j - 1)
}
}
}
}
return(data)
}
#' @param data          data.frame to split
#' @param target        string, name of the target column
#' @param train_size    double / float, value from range [0,1] which sets the
#' size of the training set
#'
#' @return              list of: train_x, train_y, test_x, test_y data.frames
#' @export
#'
#' @examples
#'
train_test_split <- function(data, target, train_size =0.6){
smp_size  <- floor(0.75 * nrow(data))
set.seed(123)
train_ind <- sample(seq_len(nrow(data)), size = smp_size)
train_x   <- data[train_ind,][!(names(data) %in% target)]
test_x    <- data[-train_ind,][!(names(data) %in% target)]
train_y   <- data[train_ind,][target]
test_y    <- data[-train_ind,][target]
return(list(
'train_x' = train_x,
'train_y' = train_y,
'test_x' = test_x,
'test_y' = test_y
))
}
#' columns with more than two unique categories only.
#'
#' @param data      data.frame to binarize
#' @param to_scale  column names or numbers which were used for scaling
#' @param target    string providing target column name
#'
#' @return          data.frame with one hot encoded categorical columns
#' @export
#'
#' @examples
one_hot_encode <- function(data, to_scale, target){
uniqueness <- lapply(data, unique)
str        <- ""
to_rm      <- c()
for (el in names(uniqueness)) {
if (length(uniqueness[[el]]) > 2 && is.character(uniqueness[[el]])) {
to_rm <- c(to_rm, el)
if (str != "") {
str <- paste(str, "+", el, sep = "")
} else{
str <- el
}
}
}
if (str != "") {
dummy  <- dummyVars(paste(" ~ ", str), data = data)
OHE_df <- data.frame(predict(dummy, newdata = data))
if (target %in% colnames(data)) {
if (!is.na(to_scale)) {
encoded_df <- cbind(data[c(names(to_scale), target)], OHE_df)
} else{
encoded_df <- cbind(data[c(target)], OHE_df)
}
}
else {
if (!is.na(to_scale)) {
encoded_df <- cbind(data[to_scale], OHE_df)
} else{
encoded_df = OHE_df
}
}
encoded_df <- encoded_df[!names(encoded_df) %in% to_rm]
return(encoded_df)
} else{
print("No columns to encode")
return(data)
}
}
#' @param to_scale     vector of column indexes or column names to scale
#' @param missing_num  additional parameter, value which describes missing value
#' for 'numeric' observations (other than NA)
#' @param missing_cat  additional parameter, value which describes missing value
#' for 'character' observations (other than NA)
#'
#' @return             preprocessed dataset ready to split.
#' @export
#'
#' @examples
preprocessing <- function(data, target, to_scale = NA, missing_num = FALSE,
missing_cat = FALSE){
data <- to_DataFrame(data, target)
data <- null_exterminator(data, target, missing_num, missing_cat)
data <- scale_continuous(data,to_scale = to_scale)
data <- binarize_categorical(data)
data <- one_hot_encode(data,to_scale,target)
return(data)
}
#' @return  random forest model ready to predict target
#' @export
#'
#' @examples
RandomForest_function <- function(data, target, num.trees = 500, max.depth = NULL,
min.node.size = NULL, mtry = NULL,
splitrule = NULL, seed = NULL){
model <- ranger(
dependent.variable.name = target,
data = data,
num.trees = num.trees,
max.depth = max.depth,
min.node.size = min.node.size,
mtry = mtry,
splitrule = splitrule,
seed = NULL
)
return(model)
}
#' @return  vector of predictions
#' @export
#'
#' @examples
RandomForest_predict <- function(model, data, target){
predict <- predict(model,data)
return(predict$predictions)
}
rf <- RandomForest_function(
data = train,
target = "price_log",
)
predictions_train <- RandomForest_predict(
model = rf,
data = train,
target  = "price_log"
)
predictions_test <- RandomForest_predict(
model = rf,
data = test,
target  = "price_log"
)
rmse(train$price_log, predictions_train)
rmse(test$price_log, predictions_test)
library(dplyr)
library(naniar)
library(splitstackshape)
library(catboost)
set.seed(123)
catboost_prepare_data <- function(data, target, continuous_names = NULL, drops = NULL, char_na = NULL){
'This function prepares given data in order to preform catboost modeling on it.
It returns the list of data frames, which are:
X_train - all independent variables from the sample of 70% of given data ,
will be used to train the model
X_test - the rest (30%) of independent variables,
which will be used to make predictions to test the accuracy of the model
y_train - 70% of the records of dependent variable, corresponding to X_train
y_test - records of dependent variable, corresponding to X_test,
which will be used to test the accuracy between actual values and predictions
Arguments:
data - the data frame to prepare
target - targeted variable of the model
continuous_names - a vector of names of continuous columns in data frame
drops - a vector of names of highly correlated columns to drop,
based on correlation matrix
char_na - character or double used to substitute NA in the data frame'
## Data cleaning - removing data with nearly all variables blank
data <- data %>% mutate_all(funs(replace(., .== char_na, NA))) %>%
na.omit()
## Encoding continuous variables by grouping
data[continuous_names] <- data %>%select(continuous_names) %>% sapply(function(x){
q <- quantile(x, probs = c(0.33,0.66,1))
x <- cut(x, breaks = c(-Inf,q[1],q[2],q[3]),
labels = c(0,1,2))
})
## Encoding character data with integer labels
data[, sapply(data, class) == 'character']<-  data %>% select(where(is.character)) %>%
sapply(function(x) as.integer(as.factor(x)) )
## Remove highly correlated variables
data <- data[ , !(names(data) %in% drops)]
## Stratifying the data
all <- stratified(data, c(target), 0.7, bothSets = TRUE)
X_train <- all$SAMP1
X_test <- all$SAMP2
y_train <- X_train %>%
pull(target) %>%
as.numeric()
print(y_train)
y_test <- X_test %>%
pull(target) %>%
as.numeric()
exclude <- c(target)
X_train <- select(X_train, -exclude) %>% sapply(function(x){as.numeric(x)})
X_test <- select(X_test, -exclude) %>% sapply(function(x){as.numeric(x)})
list <- list("X_train" = X_train, "X_test" = X_test, "y_train" = y_train, "y_test" = y_test)
return(list)
}
catboost_function <- function(X_train,y_train, params=list(loss_function = 'RMSE')){
'This function builds the catboost model with default parameters.
The outcome of the function is mentioned model.
Arguments:
X_train - all independent variables from the sample of 70% of given data ,
will be used to train the model, should be prepared by the function
`catboost_prepare_data`
y_train - 70% of the records of dependent variable, corresponding to X_train,
should be prepared by the function `catboost_prepare_data`
params - list of parameters to base the model on, as for this version: list(loss_function = "RMSE") by changing
loss_funtion you can choose what tipe of problem it is going to be solved:
for example (regression: "RMSE", binar classification: "Logloss", multiclassification: "MultiClass")'
## Build the model
train_pool <- catboost.load_pool(data = X_train, label = y_train)
model <- catboost.train(train_pool, params = params)
return(model)
}
catboost_predict <- function(model, X_test, y_test = NULL, type = 'RawFormulaVal'){
'This function returns the predicitons of given data set, based on the used model
Arguments:
model - desired catboost model built with `catboost_function`
X_test - the rest (30%) of independent variables,
which will be used to make predictions to test the accuracy of the model,
should have been prepared with `catboost_prepare_data`
y_test - y_test - records of dependent variable, corresponding to X_test,
which will be used to test the accuracy between actual values and predictions,
should have been prepared with `catboost_prepare_data`
type - for classification and multiclassification use "Class"
'
## Predict the results from X_test
test_pool <- catboost.load_pool(data = X_test, label = y_test)
predict <- catboost.predict(model,
test_pool,
prediction_type = type)
return(as.vector(predict))
}
## catboost
### Building the model
catboost_model <- catboost_function(train[,-c(21)],train$price_log, params = list(loss_function='RMSE'))
### Making the predictions
predict <- catboost_predict(catboost_model,test[,-c(21)])
### Metrics
#### MSE on predition
sqrt(sum((predict-test$price_log)^2)/length(predict))
#### MSE on mean
sqrt(sum((mean(test$price_log)-test$price_log)^2)/length(predict))
### Metrics
#### MSE on predition
sqrt(mean((predict-test$price_log)^2)/length(predict))
### Metrics
rmse(test$price_log, predict)
prepared_data <- catboost_prepare_data(data = train,target = "price_log",continuous_names = NULL, drops = drops, char_na =char_na)
prepared_data <- catboost_prepare_data(data = train,target = "price_log",continuous_names = NULL, drops = drops)
prepared_data <- catboost_prepare_data(data = train,target = "price_log",continuous_names = NULL)
sessionInfo()
